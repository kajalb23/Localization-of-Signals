{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWHKESi9DT9n",
        "outputId": "900d73d1-347f-4d35-f217-1f4b1d15f397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 40ms/step - accuracy: 0.7253 - loss: 0.8010 - val_accuracy: 0.9533 - val_loss: 0.1564\n",
            "Epoch 2/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 39ms/step - accuracy: 0.9573 - loss: 0.1463 - val_accuracy: 0.9740 - val_loss: 0.0875\n",
            "Epoch 3/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 41ms/step - accuracy: 0.9732 - loss: 0.0931 - val_accuracy: 0.9806 - val_loss: 0.0668\n",
            "Epoch 4/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 39ms/step - accuracy: 0.9785 - loss: 0.0735 - val_accuracy: 0.9815 - val_loss: 0.0645\n",
            "Epoch 5/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 38ms/step - accuracy: 0.9831 - loss: 0.0571 - val_accuracy: 0.9834 - val_loss: 0.0577\n",
            "Epoch 6/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 39ms/step - accuracy: 0.9863 - loss: 0.0479 - val_accuracy: 0.9733 - val_loss: 0.1003\n",
            "Epoch 7/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 38ms/step - accuracy: 0.9852 - loss: 0.0486 - val_accuracy: 0.9810 - val_loss: 0.0608\n",
            "Epoch 8/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 45ms/step - accuracy: 0.9880 - loss: 0.0386 - val_accuracy: 0.9876 - val_loss: 0.0489\n",
            "Epoch 9/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 41ms/step - accuracy: 0.9909 - loss: 0.0286 - val_accuracy: 0.9822 - val_loss: 0.0646\n",
            "Epoch 10/10\n",
            "\u001b[1m1920/1920\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 40ms/step - accuracy: 0.9922 - loss: 0.0275 - val_accuracy: 0.9864 - val_loss: 0.0513\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9823 - loss: 0.0603\n",
            "Test Accuracy: 0.9866\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 2. Normalize input data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# 3. Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 4. Build DNN model\n",
        "model = Sequential([\n",
        "     LSTM(128, activation='tanh', input_shape=(28, 28)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 5. Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 6. Train model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=25, validation_split=0.2)\n",
        "\n",
        "# 7. Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ]
    }
  ]
}